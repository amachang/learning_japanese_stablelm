# 作業ログ

- github repo 作った
- what i know メモを作った
- 念の為確認したが ChatGPT は Stable Diffusion も StableLM もない時代の知識らしい
- 各種必要な環境を調べる
  - StableLM Instruct の方の Huggingface のページでも読む
- 軽くサンプルコードや requirements をみて、派生的な知識を得る
- huggingface のファイル群をざっと見る
- 最終的には GKE で使うときだけ Web UI を立ち上げるみたいにしたいけど、一旦普通の Compute Engine インスタンスを立てる
  - どのくらいだろとりあえず T4 / n1-highmem-8 / Ubuntu 22.04 / 100 GB / スポットくらいな感じで。こういうとき Ubuntu じゃないほうが良いんだろうか
    - 0.36 USD/hour
      - 消し忘れたら月 30000 円くらい飛ぶ感じで
    - 慣れてるって理由だけで Ubuntu だけど。どのディストリビューションに何が入ってるか全く知らない
  - スポットだから別のゾーンじゃないと借りられないかなとかも検討してたけど、すんなり instance allocation された
- Compute Engine の instance のセットアップスクリプトは[scripts/setup\_instance](scripts/setup_instance)にまとめていく
  - nvidia driver 535
  - cuda 11.8
  - python 3.10
- 環境セットアップは [scripts/set\_env.sh](scripts/set_env.sh)
- instruct の方で遊んでみるが、データセットを見て、質問を答えることがメインな感じがした。 base の方でちょっと遊んでみるかという感じ
  - [stablelm\_sandbox/try\_base\_model.py](stablelm_sandbox/try_base_model.py)
  - [stablelm\_sandbox/try\_instruct\_model.py](stablelm_sandbox/try_instruct_model.py)
- [dialog.py](stablelm_sandbox/dialog.py) というのを作ってちょっと遊んでみた。コンテキストの抽出とか、コンテキストの渡し方とかいろんな工夫が必要そう。
